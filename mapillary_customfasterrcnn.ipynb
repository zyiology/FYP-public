{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMEPXPRCP7_q"
   },
   "source": [
    "## install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PG34XtoXOvbj"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0b-UpI3SQ-Z",
    "outputId": "a1a05347-4cc9-49c1-a313-d3a27f9915d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_xEVTepP2iJ"
   },
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': <fiftyone.zoo.datasets.ZooDatasetSplitInfo at 0x7f3dd2e8a9e0>,\n",
       " 'train': <fiftyone.zoo.datasets.ZooDatasetSplitInfo at 0x7f3dd2e8afb0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = foz.load_zoo_dataset_info('open-images-v7')\n",
    "info.downloaded_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrrAehJrPq8l",
    "outputId": "a7b13e7e-ff5f-473d-bda5-2b66af4ecd50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/zhiyi/fiftyone/open-images-v7/train' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |█████████████████| 484/484 [564.8ms elapsed, 0s remaining, 856.9 samples/s]      \n",
      "Dataset 'open-images-v7-train-484' created\n",
      "Downloading split 'validation' to '/home/zhiyi/fiftyone/open-images-v7/validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 124/124 [177.1ms elapsed, 0s remaining, 704.8 samples/s] \n",
      "Dataset 'open-images-v7-validation-124' created\n"
     ]
    }
   ],
   "source": [
    "classes_used = [\"Cat\"]\n",
    "\n",
    "# validation_dataset = foz.load_zoo_dataset(\n",
    "#     \"coco-2017\",\n",
    "#     split=\"validation\",\n",
    "#     label_types=[\"detections\"],\n",
    "#     classes=classes_used\n",
    "# )\n",
    "\n",
    "# validation_dataset_view = validation_dataset.filter_labels(\n",
    "#     \"ground_truth\",\n",
    "#     fo.ViewField(\"label\").is_in(classes_used),\n",
    "# )\n",
    "\n",
    "# train_dataset = foz.load_zoo_dataset(\n",
    "#     \"coco-2017\",\n",
    "#     split=\"train\",\n",
    "#     label_types=[\"detections\"],\n",
    "#     classes=classes_used\n",
    "# )\n",
    "\n",
    "# train_dataset_view = train_dataset.filter_labels(\n",
    "#     \"ground_truth\",\n",
    "#     fo.ViewField(\"label\").is_in(classes_used),\n",
    "# )\n",
    "\n",
    "train_dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"train\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=classes_used,\n",
    "    max_samples=484\n",
    ")\n",
    "\n",
    "train_dataset_view = train_dataset.filter_labels(\n",
    "    \"ground_truth\",\n",
    "    fo.ViewField(\"label\").is_in(classes_used),\n",
    ")\n",
    "\n",
    "validation_dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=classes_used,\n",
    "    max_samples=124\n",
    ")\n",
    "\n",
    "validation_dataset_view = validation_dataset.filter_labels(\n",
    "    \"ground_truth\",\n",
    "    fo.ViewField(\"label\").is_in(classes_used),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Detections: {\n",
      "    'detections': [\n",
      "        <Detection: {\n",
      "            'id': '65a76a554777eadf0c10a2e2',\n",
      "            'attributes': {},\n",
      "            'tags': [],\n",
      "            'label': 'Cat',\n",
      "            'bounding_box': [0.0, 0.0, 1.0, 1.0],\n",
      "            'mask': None,\n",
      "            'confidence': None,\n",
      "            'index': None,\n",
      "            'IsOccluded': False,\n",
      "            'IsTruncated': True,\n",
      "            'IsGroupOf': False,\n",
      "            'IsDepiction': False,\n",
      "            'IsInside': False,\n",
      "        }>,\n",
      "    ],\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "for sample in validation_dataset:\n",
    "    print(sample['ground_truth'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        # transforms.append(T.RandomRotation(30))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    # transforms.append(T.Resize((256,256)))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dVjnWQtoTwKq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import tv_tensors\n",
    "# import fiftyone.utils.coco as fouc\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n",
    "\n",
    "    Args:\n",
    "        fiftyone_dataset: a FiftyOne dataset or view that will be used for\n",
    "            training or testing\n",
    "        transforms (None): a list of PyTorch transforms to apply to images\n",
    "            and targets when loading\n",
    "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset\n",
    "            that contains the desired labels to load\n",
    "        classes (None): a list of class strings that are used to define the\n",
    "            mapping between class names and indices. If None, it will use\n",
    "            all classes present in the given fiftyone_dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fiftyone_dataset,\n",
    "        transforms=None,\n",
    "        gt_field=\"ground_truth\",\n",
    "        classes=None,\n",
    "    ):\n",
    "        self.samples = fiftyone_dataset\n",
    "        self.transforms = transforms\n",
    "        self.gt_field = gt_field\n",
    "\n",
    "        self.img_paths = self.samples.values(\"filepath\")\n",
    "\n",
    "        self.classes = classes\n",
    "        if not self.classes:\n",
    "            # Get list of distinct labels that exist in the view\n",
    "            self.classes = self.samples.distinct(\n",
    "                \"%s.detections.label\" % gt_field\n",
    "            )\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        # self.filtered_indices = []\n",
    "        # for sample in fiftyone_dataset:\n",
    "        #     detections = sample[gt_field].detections\n",
    "        #     if detections and any(self.is_valid_box(det.bounding_box) for det in detections):\n",
    "        #         self.filtered_indices.append(sample.filepath)\n",
    "\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "    # @staticmethod\n",
    "    # def is_valid_box(box):\n",
    "    #     # Check if the bounding box is valid (non-zero area)\n",
    "    #     x1, y1, width, height = box\n",
    "    #     x_min, x_max = min(x1,x2), max(x1,x2)\n",
    "    #     y_min, y_max = min(y1,y2), max(y1,y2)\n",
    "    #     return (x_max > x_min) and (y_max > y_min)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # img_path = self.img_paths[idx]\n",
    "        img_path = self.img_paths[idx]\n",
    "        sample = self.samples[img_path]\n",
    "        metadata = sample.metadata\n",
    "        #img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = read_image(img_path, mode=ImageReadMode.RGB)\n",
    "        img_height, img_width = img.size()[1:]        \n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        isoccluded = []\n",
    "        iscrowd = []\n",
    "        detections = sample[self.gt_field].detections\n",
    "        for det in detections:\n",
    "            category_id = self.labels_map_rev[det.label]\n",
    "            box = list(det.bounding_box)\n",
    "            #corrected_box = box.clone()\n",
    "\n",
    "            # fiftyone dataset stores bbox as [0,1] values, and as XYWH format?\n",
    "            x,y,width,height = box\n",
    "            absolute_box = [x*img_width, y*img_height, (x+width)*img_width, (y+height)*img_height]\n",
    "            \n",
    "            # box[0], box[2] = min(box[0], box[2]), max(box[0], box[2])\n",
    "            # box[1], box[3] = min(box[1], box[3]), max(box[1], box[3])\n",
    "            # absolute_box = [box[0]*img_width, box[1]*img_height, box[2]*img_width, box[3]*img_height]\n",
    "\n",
    "            area = (absolute_box[2]-absolute_box[0])*(absolute_box[3]-absolute_box[1])\n",
    "            # area = (box[2]-box[0])*(box[3]-box[1])\n",
    "            \n",
    "            # if box[2] > box[0] and box[3] > box[1]: # make sure the box is valid\n",
    "            boxes.append(absolute_box)\n",
    "            labels.append(category_id)\n",
    "            # isoccluded.append(det.IsOccluded)\n",
    "            areas.append(area)\n",
    "            iscrowd.append(0)\n",
    "\n",
    "        target = {}\n",
    "        # target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(img_height, img_width), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # target[\"attributes\"] = torch.as_tensor(isoccluded, dtype=torch.int64)\n",
    "        target[\"image_id\"] = idx #torch.as_tensor([idx])\n",
    "        target[\"area\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.filtered_indices)\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "\n",
    "\n",
    "        # image_paths = dataset.values(\"filepath\")\n",
    "        # sample = dataset[image_paths[1]]\n",
    "        # print(sample)\n",
    "        # detections = sample[\"ground_truth\"].detections\n",
    "        # metadata = sample.metadata\n",
    "        # for det in detections:\n",
    "        #     # category_id = self.labels_map_rev[det.label]\n",
    "        #     category_id = 0\n",
    "        #     print(det.label)\n",
    "        #     print(det.IsOccluded)\n",
    "        #     print(det.bounding_box)\n",
    "        #     print(type(det.bounding_box))\n",
    "            # coco_obj = fouc.COCOObject.from_label(\n",
    "            #     det, metadata, category_id=category_id,\n",
    "            # )\n",
    "            #areas.append(det.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhiyi/fiftyone/open-images-v7/train/data/0000de486dc6c49f.jpg\n",
      "[0.019608, 0.02451, 0.978758, 0.952614]\n",
      "[20.078592, 25.09824, 1002.248192, 975.476736]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BoundingBoxes([[  20.0786,   25.0982, 1002.2482,  975.4767]], format=BoundingBoxFormat.XYXY, canvas_size=(1024, 1024))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "filepaths = train_dataset_view.values('filepath')\n",
    "print(filepaths[0])\n",
    "sample = train_dataset_view[filepaths[0]]\n",
    "\n",
    "img = read_image(filepaths[0], mode=ImageReadMode.RGB)\n",
    "img_height, img_width = img.size()[1:]        \n",
    "\n",
    "boxes = []\n",
    "\n",
    "#print(sample)\n",
    "dets = sample['ground_truth'].detections\n",
    "for det in dets:\n",
    "    # print(det['bounding_box'])\n",
    "    box = list(det.bounding_box)\n",
    "    absolute_box = [box[0]*img_width, box[1]*img_height, box[2]*img_width, box[3]*img_height]\n",
    "    print(box)\n",
    "    print(absolute_box)\n",
    "    boxes.append(absolute_box)\n",
    "\n",
    "tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(img_height, img_width), dtype=torch.float32)\n",
    "\n",
    "# # Set the CUDA_LAUNCH_BLOCKING environment variable\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "IE2DLfFKQQp2",
    "outputId": "43120976-88fd-460e-ecfa-8ebc151cd0c8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import utils # downloaded from github earlier\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# image_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "# image_transforms = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])\n",
    "# train_dataset = FiftyOneTorchDataset(train_dataset_view, transforms=image_transforms, classes=classes_used)\n",
    "# validation_dataset = FiftyOneTorchDataset(validation_dataset_view, transforms=image_transforms, classes=classes_used)\n",
    "\n",
    "train_dataset = FiftyOneTorchDataset(train_dataset_view, transforms=get_transform(train=True), classes=classes_used)\n",
    "validation_dataset = FiftyOneTorchDataset(validation_dataset_view, transforms=get_transform(train=False), classes=classes_used)\n",
    "\n",
    "# Define the ratio for splitting (e.g., 80% train, 20% validation)\n",
    "# train_ratio = 0.8\n",
    "# total_samples = len(torch_dataset)\n",
    "# train_size = int(train_ratio * total_samples)\n",
    "# print(\"train_size:\", train_size)\n",
    "# validation_size = total_samples - train_size\n",
    "# print(\"validation_size:\", validation_size)\n",
    "\n",
    "# # Use random_split to create training and validation datasets\n",
    "# train_dataset, validation_dataset = random_split(torch_dataset, [train_size, validation_size])\n",
    "\n",
    "# # Create dataloaders for both the training and validation sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=utils.collate_fn)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=4, shuffle=False, collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
      "         ...,\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1412, 0.1412]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]), {'boxes': tensor([[   1.6732,   25.0982, 1003.9214, 1000.5750]]), 'labels': tensor([1]), 'image_id': 0, 'area': tensor([977669.8125]), 'iscrowd': tensor([0])})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "124\n",
      "121\n",
      "484\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for imgs, targets in validation_dataloader:\n",
    "    i+=1\n",
    "\n",
    "print(i)\n",
    "\n",
    "print(len(validation_dataset.samples))\n",
    "\n",
    "j=0\n",
    "for imgs, targets in train_dataloader:\n",
    "    j+=1\n",
    "\n",
    "print(j)\n",
    "print(len(train_dataset.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-H4MZiLLj7jd",
    "outputId": "9736bbc4-e2f5-4b3d-f250-c7917fd64f87"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/faster_rcnn_base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image\n",
    "\n",
    "def eval_image(model, epoch):\n",
    "\n",
    "    image = read_image(\"/home/zhiyi/fiftyone/coco-2017/train/data/000000027075.jpg\")\n",
    "    \n",
    "    eval_transform = get_transform(train=False)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = eval_transform(image)\n",
    "        # convert RGBA -> RGB and move to device\n",
    "        x = x[:3, ...].to(device)\n",
    "        predictions = model([x, ])\n",
    "        pred = predictions[0]\n",
    "    \n",
    "    # print(pred) \n",
    "    for key, value in pred.items():\n",
    "        pred[key] = pred[key][:3]\n",
    "    \n",
    "    # image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "    image = image[:3, ...]\n",
    "    pred_labels = [f\"cat: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "    pred_boxes = pred[\"boxes\"].long()\n",
    "    output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "    \n",
    "    # for label, box in zip(pred_labels, pred_boxes):\n",
    "    #     print(\"label:\", label, \"box:\", box)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(output_image.permute(1, 2, 0))\n",
    "    plt.savefig(f\"eval_image/epoch{epoch}.jpg\", bbox_inches='tight', pad_inches=0)  # Specify the path and filename\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_faster_rcnn as c_frcnn\n",
    "from torchvision.models.detection._utils import overwrite_eps\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "if not os.path.isfile('fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'):\n",
    "    os.system('wget https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth')\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "custom = False\n",
    "\n",
    "if custom:\n",
    "\n",
    "    model = c_frcnn.custom_fasterrcnn_resnet50_fpn(num_classes=2)#weights=torchvision.models.detection.faster_rcnn.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "    pretrained_dict = torch.load('fasterrcnn_resnet50_fpn_coco-258fb6c6.pth')  # Load the pretrained model weights\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # Filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(pretrained_dict) \n",
    "    model.load_state_dict(model_dict)\n",
    "    overwrite_eps(model, 0.0)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "else:\n",
    "\n",
    "    # CREATE BASE_MODEL\n",
    "   \n",
    "    #model = fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.faster_rcnn.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    \n",
    "    # model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=1024,num_classes=2)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     # If a GPU is available, move the model to the GPU\n",
    "#     model = model.cuda()\n",
    "\n",
    "eval_image(model, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iv1dMMbX50Z_",
    "outputId": "2e8296b5-b679-40c1-b326-7b03b8ae753e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/121]  eta: 0:01:59  lr: 0.000047  loss: 0.3614 (0.3614)  loss_classifier: 0.3143 (0.3143)  loss_box_reg: 0.0296 (0.0296)  loss_objectness: 0.0004 (0.0004)  loss_rpn_box_reg: 0.0170 (0.0170)  time: 0.9907  data: 0.0255  max mem: 4885\n",
      "Epoch: [0]  [120/121]  eta: 0:00:00  lr: 0.005000  loss: 0.0937 (0.1469)  loss_classifier: 0.0278 (0.0657)  loss_box_reg: 0.0525 (0.0596)  loss_objectness: 0.0025 (0.0039)  loss_rpn_box_reg: 0.0161 (0.0176)  time: 0.8543  data: 0.0216  max mem: 6164\n",
      "Epoch: [0] Total time: 0:01:42 (0.8456 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4114 (0.4114)  evaluator_time: 0.0012 (0.0012)  time: 0.4436  data: 0.0232  max mem: 6164\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3231 (0.3591)  evaluator_time: 0.0012 (0.0012)  time: 0.3845  data: 0.0205  max mem: 6164\n",
      "Test: Total time: 0:00:12 (0.3878 s / it)\n",
      "Averaged stats: model_time: 0.3231 (0.3591)  evaluator_time: 0.0012 (0.0012)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.661\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.947\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.753\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.662\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.746\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.746\n",
      "Epoch: [1]  [  0/121]  eta: 0:01:42  lr: 0.005000  loss: 0.1020 (0.1020)  loss_classifier: 0.0261 (0.0261)  loss_box_reg: 0.0477 (0.0477)  loss_objectness: 0.0053 (0.0053)  loss_rpn_box_reg: 0.0230 (0.0230)  time: 0.8438  data: 0.0266  max mem: 6164\n",
      "Epoch: [1]  [120/121]  eta: 0:00:00  lr: 0.005000  loss: 0.1023 (0.1035)  loss_classifier: 0.0249 (0.0299)  loss_box_reg: 0.0456 (0.0534)  loss_objectness: 0.0021 (0.0026)  loss_rpn_box_reg: 0.0173 (0.0176)  time: 0.8743  data: 0.0205  max mem: 6639\n",
      "Epoch: [1] Total time: 0:01:42 (0.8478 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4058 (0.4058)  evaluator_time: 0.0011 (0.0011)  time: 0.4364  data: 0.0233  max mem: 6639\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3225 (0.3557)  evaluator_time: 0.0011 (0.0011)  time: 0.3821  data: 0.0206  max mem: 6639\n",
      "Test: Total time: 0:00:11 (0.3842 s / it)\n",
      "Averaged stats: model_time: 0.3225 (0.3557)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.653\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.775\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.653\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.615\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.710\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.710\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.710\n",
      "Epoch: [2]  [  0/121]  eta: 0:01:40  lr: 0.005000  loss: 0.0641 (0.0641)  loss_classifier: 0.0172 (0.0172)  loss_box_reg: 0.0301 (0.0301)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0151 (0.0151)  time: 0.8342  data: 0.0181  max mem: 6639\n",
      "Epoch: [2]  [120/121]  eta: 0:00:00  lr: 0.005000  loss: 0.0744 (0.0850)  loss_classifier: 0.0191 (0.0233)  loss_box_reg: 0.0307 (0.0441)  loss_objectness: 0.0013 (0.0020)  loss_rpn_box_reg: 0.0149 (0.0155)  time: 0.8704  data: 0.0213  max mem: 6639\n",
      "Epoch: [2] Total time: 0:01:42 (0.8450 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4194 (0.4194)  evaluator_time: 0.0011 (0.0011)  time: 0.4501  data: 0.0233  max mem: 6639\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3249 (0.3626)  evaluator_time: 0.0011 (0.0011)  time: 0.3879  data: 0.0204  max mem: 6639\n",
      "Test: Total time: 0:00:12 (0.3910 s / it)\n",
      "Averaged stats: model_time: 0.3249 (0.3626)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.722\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.792\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.722\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.673\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.771\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.771\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.771\n",
      "Epoch: [3]  [  0/121]  eta: 0:01:46  lr: 0.000500  loss: 0.0678 (0.0678)  loss_classifier: 0.0166 (0.0166)  loss_box_reg: 0.0276 (0.0276)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0233 (0.0233)  time: 0.8790  data: 0.0219  max mem: 6639\n",
      "Epoch: [3]  [120/121]  eta: 0:00:00  lr: 0.000500  loss: 0.0682 (0.0682)  loss_classifier: 0.0187 (0.0181)  loss_box_reg: 0.0306 (0.0347)  loss_objectness: 0.0011 (0.0016)  loss_rpn_box_reg: 0.0117 (0.0138)  time: 0.8635  data: 0.0212  max mem: 6639\n",
      "Epoch: [3] Total time: 0:01:42 (0.8432 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4040 (0.4040)  evaluator_time: 0.0010 (0.0010)  time: 0.4346  data: 0.0233  max mem: 6639\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3144 (0.3519)  evaluator_time: 0.0010 (0.0011)  time: 0.3765  data: 0.0205  max mem: 6639\n",
      "Test: Total time: 0:00:11 (0.3803 s / it)\n",
      "Averaged stats: model_time: 0.3144 (0.3519)  evaluator_time: 0.0010 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.734\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.936\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.836\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.734\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.677\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.782\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.782\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.782\n",
      "Epoch: [4]  [  0/121]  eta: 0:01:18  lr: 0.000500  loss: 0.0918 (0.0918)  loss_classifier: 0.0215 (0.0215)  loss_box_reg: 0.0525 (0.0525)  loss_objectness: 0.0015 (0.0015)  loss_rpn_box_reg: 0.0163 (0.0163)  time: 0.6494  data: 0.0271  max mem: 6639\n",
      "Epoch: [4]  [120/121]  eta: 0:00:00  lr: 0.000500  loss: 0.0589 (0.0646)  loss_classifier: 0.0147 (0.0169)  loss_box_reg: 0.0275 (0.0325)  loss_objectness: 0.0012 (0.0017)  loss_rpn_box_reg: 0.0105 (0.0135)  time: 0.8844  data: 0.0217  max mem: 6640\n",
      "Epoch: [4] Total time: 0:01:42 (0.8446 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4112 (0.4112)  evaluator_time: 0.0010 (0.0010)  time: 0.4422  data: 0.0237  max mem: 6640\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3210 (0.3577)  evaluator_time: 0.0010 (0.0011)  time: 0.3837  data: 0.0206  max mem: 6640\n",
      "Test: Total time: 0:00:11 (0.3863 s / it)\n",
      "Averaged stats: model_time: 0.3210 (0.3577)  evaluator_time: 0.0010 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.829\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.669\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.778\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.778\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.778\n",
      "Epoch: [5]  [  0/121]  eta: 0:01:31  lr: 0.000500  loss: 0.0590 (0.0590)  loss_classifier: 0.0182 (0.0182)  loss_box_reg: 0.0249 (0.0249)  loss_objectness: 0.0049 (0.0049)  loss_rpn_box_reg: 0.0109 (0.0109)  time: 0.7563  data: 0.0250  max mem: 6640\n",
      "Epoch: [5]  [120/121]  eta: 0:00:00  lr: 0.000500  loss: 0.0529 (0.0617)  loss_classifier: 0.0157 (0.0164)  loss_box_reg: 0.0315 (0.0309)  loss_objectness: 0.0012 (0.0014)  loss_rpn_box_reg: 0.0078 (0.0129)  time: 0.8107  data: 0.0205  max mem: 7119\n",
      "Epoch: [5] Total time: 0:01:42 (0.8485 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4043 (0.4043)  evaluator_time: 0.0011 (0.0011)  time: 0.4348  data: 0.0232  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3219 (0.3559)  evaluator_time: 0.0011 (0.0011)  time: 0.3829  data: 0.0204  max mem: 7119\n",
      "Test: Total time: 0:00:11 (0.3844 s / it)\n",
      "Averaged stats: model_time: 0.3219 (0.3559)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.733\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.929\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.862\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.733\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.674\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.784\n",
      "Epoch: [6]  [  0/121]  eta: 0:02:06  lr: 0.000050  loss: 0.0851 (0.0851)  loss_classifier: 0.0274 (0.0274)  loss_box_reg: 0.0473 (0.0473)  loss_objectness: 0.0014 (0.0014)  loss_rpn_box_reg: 0.0091 (0.0091)  time: 1.0430  data: 0.0234  max mem: 7119\n",
      "Epoch: [6]  [120/121]  eta: 0:00:00  lr: 0.000050  loss: 0.0526 (0.0604)  loss_classifier: 0.0156 (0.0164)  loss_box_reg: 0.0259 (0.0299)  loss_objectness: 0.0006 (0.0015)  loss_rpn_box_reg: 0.0075 (0.0126)  time: 0.8407  data: 0.0212  max mem: 7119\n",
      "Epoch: [6] Total time: 0:01:41 (0.8402 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4002 (0.4002)  evaluator_time: 0.0010 (0.0010)  time: 0.4310  data: 0.0236  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3230 (0.3538)  evaluator_time: 0.0011 (0.0011)  time: 0.3811  data: 0.0206  max mem: 7119\n",
      "Test: Total time: 0:00:11 (0.3825 s / it)\n",
      "Averaged stats: model_time: 0.3230 (0.3538)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.734\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.930\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.848\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.734\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.786\n",
      "Epoch: [7]  [  0/121]  eta: 0:01:35  lr: 0.000050  loss: 0.0639 (0.0639)  loss_classifier: 0.0151 (0.0151)  loss_box_reg: 0.0215 (0.0215)  loss_objectness: 0.0073 (0.0073)  loss_rpn_box_reg: 0.0200 (0.0200)  time: 0.7914  data: 0.0225  max mem: 7119\n",
      "Epoch: [7]  [120/121]  eta: 0:00:00  lr: 0.000050  loss: 0.0472 (0.0602)  loss_classifier: 0.0137 (0.0162)  loss_box_reg: 0.0237 (0.0302)  loss_objectness: 0.0007 (0.0014)  loss_rpn_box_reg: 0.0089 (0.0124)  time: 0.8388  data: 0.0215  max mem: 7119\n",
      "Epoch: [7] Total time: 0:01:42 (0.8456 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4058 (0.4058)  evaluator_time: 0.0010 (0.0010)  time: 0.4364  data: 0.0233  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3233 (0.3580)  evaluator_time: 0.0011 (0.0011)  time: 0.3855  data: 0.0205  max mem: 7119\n",
      "Test: Total time: 0:00:11 (0.3865 s / it)\n",
      "Averaged stats: model_time: 0.3233 (0.3580)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.734\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.835\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.734\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.678\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.785\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.785\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.785\n",
      "Epoch: [8]  [  0/121]  eta: 0:01:18  lr: 0.000050  loss: 0.1198 (0.1198)  loss_classifier: 0.0305 (0.0305)  loss_box_reg: 0.0813 (0.0813)  loss_objectness: 0.0034 (0.0034)  loss_rpn_box_reg: 0.0046 (0.0046)  time: 0.6511  data: 0.0210  max mem: 7119\n",
      "Epoch: [8]  [120/121]  eta: 0:00:00  lr: 0.000050  loss: 0.0511 (0.0593)  loss_classifier: 0.0142 (0.0158)  loss_box_reg: 0.0236 (0.0293)  loss_objectness: 0.0007 (0.0017)  loss_rpn_box_reg: 0.0115 (0.0125)  time: 0.8531  data: 0.0205  max mem: 7119\n",
      "Epoch: [8] Total time: 0:01:39 (0.8264 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4070 (0.4070)  evaluator_time: 0.0015 (0.0015)  time: 0.4380  data: 0.0232  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3272 (0.3636)  evaluator_time: 0.0011 (0.0011)  time: 0.3905  data: 0.0205  max mem: 7119\n",
      "Test: Total time: 0:00:12 (0.3922 s / it)\n",
      "Averaged stats: model_time: 0.3272 (0.3636)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.731\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.784\n",
      "Epoch: [9]  [  0/121]  eta: 0:01:39  lr: 0.000005  loss: 0.0430 (0.0430)  loss_classifier: 0.0169 (0.0169)  loss_box_reg: 0.0235 (0.0235)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0024 (0.0024)  time: 0.8257  data: 0.0316  max mem: 7119\n",
      "Epoch: [9]  [120/121]  eta: 0:00:00  lr: 0.000005  loss: 0.0573 (0.0590)  loss_classifier: 0.0164 (0.0158)  loss_box_reg: 0.0265 (0.0292)  loss_objectness: 0.0007 (0.0014)  loss_rpn_box_reg: 0.0090 (0.0127)  time: 0.8695  data: 0.0222  max mem: 7119\n",
      "Epoch: [9] Total time: 0:01:41 (0.8413 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4075 (0.4075)  evaluator_time: 0.0010 (0.0010)  time: 0.4382  data: 0.0235  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3256 (0.3611)  evaluator_time: 0.0011 (0.0011)  time: 0.3879  data: 0.0203  max mem: 7119\n",
      "Test: Total time: 0:00:12 (0.3894 s / it)\n",
      "Averaged stats: model_time: 0.3256 (0.3611)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.731\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.784\n",
      "Epoch: [10]  [  0/121]  eta: 0:01:50  lr: 0.000005  loss: 0.0666 (0.0666)  loss_classifier: 0.0212 (0.0212)  loss_box_reg: 0.0253 (0.0253)  loss_objectness: 0.0013 (0.0013)  loss_rpn_box_reg: 0.0189 (0.0189)  time: 0.9142  data: 0.0263  max mem: 7119\n",
      "Epoch: [10]  [120/121]  eta: 0:00:00  lr: 0.000005  loss: 0.0503 (0.0606)  loss_classifier: 0.0143 (0.0165)  loss_box_reg: 0.0242 (0.0303)  loss_objectness: 0.0007 (0.0015)  loss_rpn_box_reg: 0.0078 (0.0124)  time: 0.8527  data: 0.0208  max mem: 7119\n",
      "Epoch: [10] Total time: 0:01:42 (0.8473 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4067 (0.4067)  evaluator_time: 0.0010 (0.0010)  time: 0.4373  data: 0.0234  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3195 (0.3545)  evaluator_time: 0.0010 (0.0011)  time: 0.3795  data: 0.0203  max mem: 7119\n",
      "Test: Total time: 0:00:11 (0.3829 s / it)\n",
      "Averaged stats: model_time: 0.3195 (0.3545)  evaluator_time: 0.0010 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.782\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.782\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.782\n",
      "Epoch: [11]  [  0/121]  eta: 0:02:03  lr: 0.000005  loss: 0.0529 (0.0529)  loss_classifier: 0.0076 (0.0076)  loss_box_reg: 0.0176 (0.0176)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0272 (0.0272)  time: 1.0241  data: 0.0209  max mem: 7119\n",
      "Epoch: [11]  [120/121]  eta: 0:00:00  lr: 0.000005  loss: 0.0545 (0.0597)  loss_classifier: 0.0142 (0.0162)  loss_box_reg: 0.0253 (0.0297)  loss_objectness: 0.0012 (0.0015)  loss_rpn_box_reg: 0.0133 (0.0124)  time: 0.8396  data: 0.0212  max mem: 7119\n",
      "Epoch: [11] Total time: 0:01:41 (0.8370 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.3995 (0.3995)  evaluator_time: 0.0010 (0.0010)  time: 0.4298  data: 0.0231  max mem: 7119\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3188 (0.3538)  evaluator_time: 0.0011 (0.0011)  time: 0.3790  data: 0.0203  max mem: 7119\n",
      "Test: Total time: 0:00:11 (0.3823 s / it)\n",
      "Averaged stats: model_time: 0.3188 (0.3538)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [12]  [  0/121]  eta: 0:01:38  lr: 0.000000  loss: 0.0507 (0.0507)  loss_classifier: 0.0181 (0.0181)  loss_box_reg: 0.0280 (0.0280)  loss_objectness: 0.0006 (0.0006)  loss_rpn_box_reg: 0.0039 (0.0039)  time: 0.8141  data: 0.0188  max mem: 7119\n",
      "Epoch: [12]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0620 (0.0596)  loss_classifier: 0.0188 (0.0163)  loss_box_reg: 0.0293 (0.0295)  loss_objectness: 0.0008 (0.0013)  loss_rpn_box_reg: 0.0096 (0.0125)  time: 0.7739  data: 0.0207  max mem: 7120\n",
      "Epoch: [12] Total time: 0:01:38 (0.8117 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:12  model_time: 0.3859 (0.3859)  evaluator_time: 0.0010 (0.0010)  time: 0.4171  data: 0.0239  max mem: 7120\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3065 (0.3395)  evaluator_time: 0.0011 (0.0011)  time: 0.3662  data: 0.0204  max mem: 7120\n",
      "Test: Total time: 0:00:11 (0.3680 s / it)\n",
      "Averaged stats: model_time: 0.3065 (0.3395)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [13]  [  0/121]  eta: 0:01:26  lr: 0.000000  loss: 0.0493 (0.0493)  loss_classifier: 0.0097 (0.0097)  loss_box_reg: 0.0280 (0.0280)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0112 (0.0112)  time: 0.7111  data: 0.0216  max mem: 7120\n",
      "Epoch: [13]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0528 (0.0597)  loss_classifier: 0.0140 (0.0162)  loss_box_reg: 0.0256 (0.0295)  loss_objectness: 0.0006 (0.0014)  loss_rpn_box_reg: 0.0081 (0.0127)  time: 0.8614  data: 0.0227  max mem: 7120\n",
      "Epoch: [13] Total time: 0:01:38 (0.8155 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.3988 (0.3988)  evaluator_time: 0.0010 (0.0010)  time: 0.4294  data: 0.0232  max mem: 7120\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3145 (0.3480)  evaluator_time: 0.0010 (0.0011)  time: 0.3754  data: 0.0209  max mem: 7120\n",
      "Test: Total time: 0:00:11 (0.3768 s / it)\n",
      "Averaged stats: model_time: 0.3145 (0.3480)  evaluator_time: 0.0010 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [14]  [  0/121]  eta: 0:01:17  lr: 0.000000  loss: 0.0375 (0.0375)  loss_classifier: 0.0104 (0.0104)  loss_box_reg: 0.0153 (0.0153)  loss_objectness: 0.0007 (0.0007)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.6403  data: 0.0221  max mem: 7120\n",
      "Epoch: [14]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0537 (0.0592)  loss_classifier: 0.0127 (0.0154)  loss_box_reg: 0.0265 (0.0294)  loss_objectness: 0.0008 (0.0014)  loss_rpn_box_reg: 0.0081 (0.0130)  time: 0.8356  data: 0.0219  max mem: 7120\n",
      "Epoch: [14] Total time: 0:01:38 (0.8152 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:12  model_time: 0.3654 (0.3654)  evaluator_time: 0.0010 (0.0010)  time: 0.3956  data: 0.0230  max mem: 7120\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3052 (0.3295)  evaluator_time: 0.0010 (0.0011)  time: 0.3591  data: 0.0204  max mem: 7120\n",
      "Test: Total time: 0:00:11 (0.3579 s / it)\n",
      "Averaged stats: model_time: 0.3052 (0.3295)  evaluator_time: 0.0010 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [15]  [  0/121]  eta: 0:01:21  lr: 0.000000  loss: 0.0447 (0.0447)  loss_classifier: 0.0130 (0.0130)  loss_box_reg: 0.0208 (0.0208)  loss_objectness: 0.0006 (0.0006)  loss_rpn_box_reg: 0.0103 (0.0103)  time: 0.6762  data: 0.0189  max mem: 7120\n",
      "Epoch: [15]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0473 (0.0589)  loss_classifier: 0.0127 (0.0156)  loss_box_reg: 0.0235 (0.0291)  loss_objectness: 0.0007 (0.0013)  loss_rpn_box_reg: 0.0097 (0.0128)  time: 0.7764  data: 0.0222  max mem: 7120\n",
      "Epoch: [15] Total time: 0:01:34 (0.7796 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:12  model_time: 0.3800 (0.3800)  evaluator_time: 0.0010 (0.0010)  time: 0.4102  data: 0.0230  max mem: 7120\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3071 (0.3368)  evaluator_time: 0.0011 (0.0011)  time: 0.3618  data: 0.0204  max mem: 7120\n",
      "Test: Total time: 0:00:11 (0.3653 s / it)\n",
      "Averaged stats: model_time: 0.3071 (0.3368)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [16]  [  0/121]  eta: 0:01:43  lr: 0.000000  loss: 0.0479 (0.0479)  loss_classifier: 0.0164 (0.0164)  loss_box_reg: 0.0220 (0.0220)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0093 (0.0093)  time: 0.8520  data: 0.0189  max mem: 7120\n",
      "Epoch: [16]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0577 (0.0601)  loss_classifier: 0.0152 (0.0162)  loss_box_reg: 0.0255 (0.0299)  loss_objectness: 0.0009 (0.0013)  loss_rpn_box_reg: 0.0087 (0.0128)  time: 0.7621  data: 0.0218  max mem: 7120\n",
      "Epoch: [16] Total time: 0:01:35 (0.7881 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:12  model_time: 0.3665 (0.3665)  evaluator_time: 0.0010 (0.0010)  time: 0.3967  data: 0.0230  max mem: 7120\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3061 (0.3343)  evaluator_time: 0.0011 (0.0011)  time: 0.3594  data: 0.0204  max mem: 7120\n",
      "Test: Total time: 0:00:11 (0.3627 s / it)\n",
      "Averaged stats: model_time: 0.3061 (0.3343)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [17]  [  0/121]  eta: 0:01:48  lr: 0.000000  loss: 0.0778 (0.0778)  loss_classifier: 0.0174 (0.0174)  loss_box_reg: 0.0333 (0.0333)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0254 (0.0254)  time: 0.8959  data: 0.0350  max mem: 7120\n",
      "Epoch: [17]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0480 (0.0597)  loss_classifier: 0.0117 (0.0159)  loss_box_reg: 0.0229 (0.0296)  loss_objectness: 0.0007 (0.0014)  loss_rpn_box_reg: 0.0110 (0.0128)  time: 0.8310  data: 0.0219  max mem: 7120\n",
      "Epoch: [17] Total time: 0:01:35 (0.7860 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:12  model_time: 0.3863 (0.3863)  evaluator_time: 0.0010 (0.0010)  time: 0.4165  data: 0.0230  max mem: 7120\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3132 (0.3402)  evaluator_time: 0.0011 (0.0011)  time: 0.3672  data: 0.0208  max mem: 7120\n",
      "Test: Total time: 0:00:11 (0.3690 s / it)\n",
      "Averaged stats: model_time: 0.3132 (0.3402)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [18]  [  0/121]  eta: 0:01:25  lr: 0.000000  loss: 0.0652 (0.0652)  loss_classifier: 0.0231 (0.0231)  loss_box_reg: 0.0355 (0.0355)  loss_objectness: 0.0022 (0.0022)  loss_rpn_box_reg: 0.0044 (0.0044)  time: 0.7079  data: 0.0231  max mem: 7120\n",
      "Epoch: [18]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0576 (0.0589)  loss_classifier: 0.0161 (0.0158)  loss_box_reg: 0.0303 (0.0294)  loss_objectness: 0.0008 (0.0013)  loss_rpn_box_reg: 0.0092 (0.0122)  time: 0.8155  data: 0.0221  max mem: 7122\n",
      "Epoch: [18] Total time: 0:01:37 (0.8055 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4094 (0.4094)  evaluator_time: 0.0010 (0.0010)  time: 0.4402  data: 0.0235  max mem: 7122\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3226 (0.3567)  evaluator_time: 0.0011 (0.0011)  time: 0.3824  data: 0.0207  max mem: 7122\n",
      "Test: Total time: 0:00:11 (0.3854 s / it)\n",
      "Averaged stats: model_time: 0.3226 (0.3567)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
      "Epoch: [19]  [  0/121]  eta: 0:01:30  lr: 0.000000  loss: 0.0608 (0.0608)  loss_classifier: 0.0140 (0.0140)  loss_box_reg: 0.0301 (0.0301)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.0156 (0.0156)  time: 0.7473  data: 0.0214  max mem: 7122\n",
      "Epoch: [19]  [120/121]  eta: 0:00:00  lr: 0.000000  loss: 0.0560 (0.0589)  loss_classifier: 0.0165 (0.0160)  loss_box_reg: 0.0244 (0.0293)  loss_objectness: 0.0013 (0.0013)  loss_rpn_box_reg: 0.0093 (0.0124)  time: 0.8402  data: 0.0222  max mem: 7123\n",
      "Epoch: [19] Total time: 0:01:36 (0.7949 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/31]  eta: 0:00:13  model_time: 0.4155 (0.4155)  evaluator_time: 0.0011 (0.0011)  time: 0.4478  data: 0.0249  max mem: 7123\n",
      "Test:  [30/31]  eta: 0:00:00  model_time: 0.3229 (0.3602)  evaluator_time: 0.0011 (0.0011)  time: 0.3867  data: 0.0211  max mem: 7123\n",
      "Test: Total time: 0:00:12 (0.3894 s / it)\n",
      "Averaged stats: model_time: 0.3229 (0.3602)  evaluator_time: 0.0011 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.933\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.730\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.783\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n"
     ]
    }
   ],
   "source": [
    "import engine\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# figure out why class_logits is still rly long\n",
    "\n",
    "if custom:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train the model for one epoch\n",
    "        train_metrics = c_frcnn.train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=500)\n",
    "    \n",
    "        writer.add_scalar('Loss/train', train_metrics.loss.avg, epoch)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        validation_metrics = c_frcnn.evaluate(model, validation_dataloader, device=device)\n",
    "        writer.add_scalar('mAP/validation', validation_metrics.coco_eval['bbox'].stats[0], epoch)\n",
    "        \n",
    "        # save image of example\n",
    "        eval_image(model, epoch)\n",
    "\n",
    "else:\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # train the model for one epoch\n",
    "        train_metrics=engine.train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=250)\n",
    "        writer.add_scalar('Loss/train', train_metrics.loss.global_avg, epoch)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        validation_metrics=engine.evaluate(model, validation_dataloader, device=device)\n",
    "        writer.add_scalar('mAP/validation', validation_metrics.coco_eval['bbox'].stats[0], epoch)\n",
    "    \n",
    "        eval_image(model, epoch)\n",
    "\n",
    "        # Additional information\n",
    "        checkpoint_file = f\"./checkpoints_stop/model_{epoch}.pt\"\n",
    "        \n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': train_metrics.loss.global_avg\n",
    "                    }, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# print(train_metrics.loss.avg)\n",
    "# print(validation_metrics.coco_eval['bbox'].stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END\n",
    "\n",
    "# print(train_metrics.loss.global_avg)\n",
    "shutdown=False\n",
    "\n",
    "if shutdown:\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.system('shutdown -h now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping  \n",
    "# plot bbox from model\n",
    "#file = \"/home/zhiyi/fiftyone/open-images-v7/validation/data/0f426802e98a8964.jpg\"\n",
    "#file = \"/home/zhiyi/fiftyone/open-images-v7/train/data/0ca0f9c12dde03e0.jpg\"\n",
    "file = '/home/zhiyi/fiftyone/coco-2017/train/data/000000000064.jpg'\n",
    "image = read_image(file)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_transform = get_transform(train=False)\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = predictions[0]\n",
    "\n",
    "# print(pred)\n",
    "\n",
    "for key, value in pred.items():\n",
    "    pred[key] = pred[key][:1]\n",
    "\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"cat: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "\n",
    "for label, box in zip(pred_labels, pred_boxes):\n",
    "    print(\"label:\", label, \"box:\", box)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping \n",
    "#TARGET BBOX\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "def plot_image_with_boxes(dataset, image_file_name):\n",
    "    # Find the sample in the dataset\n",
    "    sample = next((s for s in dataset.samples if s.filepath == image_file_name), None)\n",
    "    if sample is None:\n",
    "        print(f\"Image {image_file_name} not found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(sample.filepath)\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Plot each bounding box\n",
    "    detections = sample[dataset.gt_field].detections\n",
    "    for det in detections:\n",
    "        # Scale bounding box coordinates to image size\n",
    "        x, y, width, height = det.bounding_box\n",
    "        x, width = x * img_width, width * img_width\n",
    "        y, height = y * img_height, height * img_height\n",
    "\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, det.label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_image_with_boxes(train_dataset, \"/home/zhiyi/fiftyone/coco-2017/train/data/000000000064.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping \n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_dataset = FiftyOneTorchDataset(validation_dataset_view, transforms=T.ToDtype(torch.float, scale=True), classes=classes_used)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for images, targets in test_dataloader:\n",
    "    tensor_image = images[0]\n",
    "    # print(tensor_image.size())\n",
    "    _, img_height, img_width = tensor_image.size()\n",
    "    target = targets[0]\n",
    "    tensor_image = tensor_image.permute(1, 2, 0)\n",
    "    numpy_image = tensor_image.numpy()\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(numpy_image)\n",
    "    \n",
    "    image_id = target['image_id']\n",
    "    image_path = test_dataset.img_paths[image_id]\n",
    "    # print(validation_dataset_view[image_path])\n",
    "    \n",
    "    # img = Image.open(image_path)\n",
    "    \n",
    "    for box in target['boxes']:\n",
    "        # Scale bounding box coordinates to image size\n",
    "        x1,y1,x2,y2 = box\n",
    "        width = x2-x1\n",
    "        height=y2-y1\n",
    "        # x1,y1,width,height=box\n",
    "\n",
    "        # print((x1,y1,width,height))\n",
    "        \n",
    "        \n",
    "        # x, y, width, height = box\n",
    "        # x, width = x * img_width, width * img_width\n",
    "        # y, height = y * img_height, height * img_height\n",
    "\n",
    "        rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, 'cat', color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "    i+=1\n",
    "    if i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PnGPp5H5Rzh-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping \n",
    "img = read_image(\"/home/zhiyi/fiftyone/open-images-v7/validation/data/0f426802e98a8964.jpg\")\n",
    "eval_transform = get_transform(train=False)\n",
    "print(img.max())\n",
    "print(img.min())\n",
    "print(img.dtype)\n",
    "img = eval_transform(img)\n",
    "print(img.max())\n",
    "print(img.min())\n",
    "# img_width, img_height = img.size()[1:]\n",
    "# print(img.size())\n",
    "# print(img_width)\n",
    "# print(type(img_width))\n",
    "\n",
    "# num_epochs = 3\n",
    "\n",
    "# # train on the GPU or on the CPU, if a GPU is not available\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     params,\n",
    "#     lr=0.005,\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0.0005\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#   train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yjc-is8KfVcs",
    "outputId": "9434ab11-d45c-4660-834a-6b595ef71d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping \n",
    "image_paths = dataset.values(\"filepath\")\n",
    "sample = dataset[image_paths[0]]\n",
    "print(sample)\n",
    "detections = sample[\"ground_truth\"].detections\n",
    "metadata = sample.metadata\n",
    "for det in detections:\n",
    "    # category_id = self.labels_map_rev[det.label]\n",
    "    category_id = 0\n",
    "    print(det.label)\n",
    "    print(det.IsOccluded)\n",
    "    print(list(det.bounding_box))\n",
    "    print(type(list(det.bounding_box)))\n",
    "    print(torch.as_tensor(list(det.bounding_box))[1])\n",
    "\n",
    "img = Image.open(image_paths[0]).convert(\"RGB\")\n",
    "img = torchvision.transforms.ToTensor()(img)\n",
    "print(type(img))\n",
    "    # coco_obj = fouc.COCOObject.from_label(\n",
    "    #     det, metadata, category_id=category_id,\n",
    "    # )\n",
    "    # x, y, w, h = coco_obj.bbox\n",
    "    # print(coco_obj.category_id)\n",
    "    # print(coco_obj.area)\n",
    "    # print(coco_obj.isoccluded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping \n",
    "img_path = '/home/zhiyi/fiftyone/coco-2017/train/data/000000000064.jpg'\n",
    "img = read_image(img_path, mode=ImageReadMode.RGB)\n",
    "img_height, img_width = img.size()[1:]   \n",
    "print(img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping \n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import tv_tensors\n",
    "# import fiftyone.utils.coco as fouc\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "class FiftyOneOpenImagesTorchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n",
    "\n",
    "    Args:\n",
    "        fiftyone_dataset: a FiftyOne dataset or view that will be used for\n",
    "            training or testing\n",
    "        transforms (None): a list of PyTorch transforms to apply to images\n",
    "            and targets when loading\n",
    "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset\n",
    "            that contains the desired labels to load\n",
    "        classes (None): a list of class strings that are used to define the\n",
    "            mapping between class names and indices. If None, it will use\n",
    "            all classes present in the given fiftyone_dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fiftyone_dataset,\n",
    "        transforms=None,\n",
    "        gt_field=\"ground_truth\",\n",
    "        classes=None,\n",
    "    ):\n",
    "        self.samples = fiftyone_dataset\n",
    "        self.transforms = transforms\n",
    "        self.gt_field = gt_field\n",
    "\n",
    "        self.img_paths = self.samples.values(\"filepath\")\n",
    "\n",
    "        self.classes = classes\n",
    "        if not self.classes:\n",
    "            # Get list of distinct labels that exist in the view\n",
    "            self.classes = self.samples.distinct(\n",
    "                \"%s.detections.label\" % gt_field\n",
    "            )\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        self.filtered_indices = []\n",
    "        for sample in fiftyone_dataset:\n",
    "            detections = sample[gt_field].detections\n",
    "            if detections and any(self.is_valid_box(det.bounding_box) for det in detections):\n",
    "                self.filtered_indices.append(sample.filepath)\n",
    "\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_box(box):\n",
    "        # Check if the bounding box is valid (non-zero area)\n",
    "        x1, y1, x2, y2 = box\n",
    "        x_min, x_max = min(x1,x2), max(x1,x2)\n",
    "        y_min, y_max = min(y1,y2), max(y1,y2)\n",
    "        return (x_max > x_min) and (y_max > y_min)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # img_path = self.img_paths[idx]\n",
    "        img_path = self.filtered_indices[idx]\n",
    "        sample = self.samples[img_path]\n",
    "        metadata = sample.metadata\n",
    "        #img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = read_image(img_path, mode=ImageReadMode.RGB)\n",
    "        img_height, img_width = img.size()[1:]        \n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        isoccluded = []\n",
    "        iscrowd = []\n",
    "        detections = sample[self.gt_field].detections\n",
    "        for det in detections:\n",
    "            category_id = self.labels_map_rev[det.label]\n",
    "            box = list(det.bounding_box)\n",
    "            #corrected_box = box.clone()\n",
    "\n",
    "            # fiftyone dataset stores bbox as [0,1] values, and as XYWH format?\n",
    "            \n",
    "            box[0], box[2] = min(box[0], box[2]), max(box[0], box[2])\n",
    "            box[1], box[3] = min(box[1], box[3]), max(box[1], box[3])\n",
    "            absolute_box = [box[0]*img_width, box[1]*img_height, box[2]*img_width, box[3]*img_height]\n",
    "\n",
    "            area = (absolute_box[2]-absolute_box[0])*(absolute_box[3]-absolute_box[1])\n",
    "            # area = (box[2]-box[0])*(box[3]-box[1])\n",
    "            \n",
    "            if box[2] > box[0] and box[3] > box[1]: # make sure the box is valid\n",
    "                boxes.append(absolute_box)\n",
    "                labels.append(category_id)\n",
    "                # isoccluded.append(det.IsOccluded)\n",
    "                areas.append(area)\n",
    "                iscrowd.append(0)\n",
    "\n",
    "        target = {}\n",
    "        # target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(img_height, img_width), dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # target[\"attributes\"] = torch.as_tensor(isoccluded, dtype=torch.int64)\n",
    "        target[\"image_id\"] = idx #torch.as_tensor([idx])\n",
    "        target[\"area\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_indices)\n",
    "        # return len(self.img_paths)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "\n",
    "\n",
    "        # image_paths = dataset.values(\"filepath\")\n",
    "        # sample = dataset[image_paths[1]]\n",
    "        # print(sample)\n",
    "        # detections = sample[\"ground_truth\"].detections\n",
    "        # metadata = sample.metadata\n",
    "        # for det in detections:\n",
    "        #     # category_id = self.labels_map_rev[det.label]\n",
    "        #     category_id = 0\n",
    "        #     print(det.label)\n",
    "        #     print(det.IsOccluded)\n",
    "        #     print(det.bounding_box)\n",
    "        #     print(type(det.bounding_box))\n",
    "            # coco_obj = fouc.COCOObject.from_label(\n",
    "            #     det, metadata, category_id=category_id,\n",
    "            # )\n",
    "            #areas.append(det.area)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
